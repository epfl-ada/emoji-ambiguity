{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from p_tqdm import p_map\n",
    "\n",
    "from emoji import unicode_codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.data.utils import parallelize_dataframe, find_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "path = \"/home/jczestochowska/workspace/dlab/emoji-ambiguity/data/external/emojitweets-01-04-2018.txt.gz\"\n",
    "tweets = []\n",
    "with gzip.open(path, mode='rt', encoding='utf-8', newline='\\n') as f:\n",
    "    for line in f.readlines():\n",
    "        tweets.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoji_tweets = pd.read_table(\"/home/jczestochowska/workspace/dlab/emoji-ambiguity/data/external/emojitweets-01-04-2018.txt.gz\",\\\n",
    "#                              header=None, lineterminator='\\n', encoding='utf-8').tolist()\n",
    "# emoji_tweets = emoji_tweets.rename({0: \"tweet\"}, axis=1)\n",
    "# emoji_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emojis(tweet):\n",
    "    emojis = set()\n",
    "    for emoji in find_emojis(tweet):\n",
    "        emojis.add(emoji)\n",
    "    return emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emojis = set.union(*p_map(get_emojis, tweets, num_cpus=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(4)\n",
    "start = time.time()\n",
    "out = set.union(*pool.map(get_emojis, emoji_tweets))\n",
    "pool.close()\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = set()\n",
    "for _, row in tqdm(emoji_tweets.iterrows()):\n",
    "    for emoji in find_emojis(row.tweet):\n",
    "        emojis.add(emoji)\n",
    "len(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_tweets['emojis'] = parallelize_dataframe(emoji_tweets, apply_find_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add space between any consecutive emojis so that they are treated as separate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# emoji_tweets.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "# doc = nlp('We are all same ğŸ‘¶ğŸ‘¶ğŸ»ğŸ‘¶ğŸ¼ğŸ‘¶ğŸ½ğŸ‘¶ğŸ¾ğŸ‘¶ğŸ¿ but different in skin colors!')\n",
    "doc = nlp(\"ğŸ’°ğŸš¶ğŸ»â€â™€ï¸ğŸš¶ğŸ»â€â™€ï¸ğŸš¶ğŸ»â€â™€ï¸ğŸ’ƒğŸ»ğŸ‘  Let em know ğŸ‘ğŸ¾\")\n",
    "\n",
    "tokenized = [token.text for token in doc]\n",
    "processed = \" \".join(tokenized)\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# s =\"ğŸ’°ğŸš¶ğŸ»â€â™€ï¸ğŸš¶ğŸ»â€â™€ï¸ğŸš¶ğŸ»â€â™€ï¸ğŸ’ƒğŸ»ğŸ‘  Let em know ğŸ‘ğŸ¾\"\n",
    "# tokenizer.add_tokens([\"ğŸ’°\", \"ğŸš¶ğŸ»â€â™€ï¸\", \"ğŸ’ƒ\", \"ğŸ‘ \", \"ğŸ‘ğŸ¾\"])\n",
    "print(tokenizer.tokenize(processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_emoji_indices(row):\n",
    "    out, emoji_token = [], '[UNK]'\n",
    "    tokenized = np.array(tokenizer.tokenize(row.tweet))\n",
    "    emojis = find_emojis(row.tweet)\n",
    "    ii = np.where(tokenized == emoji_token)[0]\n",
    "    return list(zip(emojis, ii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_emoji_indices(emoji_tweets.iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_tweets.iloc[3].tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = \"2ï¸âƒ£4ï¸âƒ£ hours 'til our schedule drops!\"\n",
    "row = 'I CANT BREATIUHW ğŸ’€ğŸ’€ğŸ’€'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_emojis(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_emojis(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "row.index(find_emojis(row)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "row[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = \"2ï¸âƒ£4ï¸âƒ£ hours 'til our schedule drops!\"\n",
    "# tokenizer.tokenize(row)\n",
    "# emojis = find_emojis(row)\n",
    "# emojis\n",
    "# emojis = find_emojis(row.tweet)\n",
    "# ii = np.where(tokenized == emoji_token)[0]\n",
    "# return list(zip(emojis, ii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = \"2ï¸âƒ£4ï¸âƒ£ hours  'til  our  schedule  drops!\"\n",
    "tokenizer.tokenize(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Squad arriving for Game 2 ğŸš€\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.tokenize(text)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.tokenize(text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][:, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-defense",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
