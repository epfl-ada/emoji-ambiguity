{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nominated-ordinary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=8)\n",
    "tqdm.pandas()\n",
    "\n",
    "from settings import AMBIGUITY_PATH, AMBIGUITY_CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accessible-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.utils import save_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "corrected-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/scratch/czestoch/sampled_tweets.txt.gz\"\n",
    "tweets = pd.read_csv(path, header=0, lineterminator='\\n', encoding='utf-8')['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "substantial-congo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93390"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "noticed-terrorism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    *⃣🆘PLEASE   🆘WE WILL NOT BE SILENT😡plz watch📼 ...\n",
       "1       *⃣PLEASE   KILL RATE⚠️RESCUE ONLY⚠️Thank you*⃣\n",
       "2    *⃣💠*⃣💠*⃣💠*⃣💠*⃣💠*⃣💠 How to TRIGGER a LIBERAL wi...\n",
       "3    \"SAFFRON\" 🧡Sweet male puppy 3 months 11.5 lbs ...\n",
       "4          *⃣PLEASE   3 DUMPED IN DROP BOX😡DIES💉2/22*⃣\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continuous-framework",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets = tweets[:1000]\n",
    "# len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chinese-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(text):\n",
    "    return add_spaces_between_emojis(emoji.demojize(text))\n",
    "\n",
    "def add_spaces_between_emojis(demojified_text):\n",
    "    new_text = []\n",
    "    colons = []\n",
    "    for char in demojified_text:\n",
    "        if char == \":\":\n",
    "            if colons:\n",
    "                new_text.append(char + \" \")\n",
    "                colons.pop()\n",
    "            else:\n",
    "                colons.append(char)\n",
    "                new_text.append(\" \" + char)\n",
    "        else:\n",
    "            new_text.append(char)\n",
    "    return ''.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "current-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.parallel_apply(preprocess_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fitted-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_emojis = pd.read_csv(AMBIGUITY_CLUSTER).emoji.unique()\n",
    "all_emojis = pd.read_csv(AMBIGUITY_PATH).emoji.unique()\n",
    "all_emojis = list(map(lambda x: emoji.demojize(x), all_emojis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hybrid-maldives",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 of our emojis are in this model\n",
      "Original number of tokens: 64001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens after extension: 64884\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "emojis_in_tokenizer = {}\n",
    "for em in all_emojis:\n",
    "    emoji_tensor = tokenizer(em, return_tensors='pt')['input_ids']\n",
    "    emoji_vocab_idx = emoji_tensor[0][1].item()\n",
    "    # if size is 3 it means emoji token was not splitted so it is a known token,\n",
    "    # [start] [emoji] [stop]\n",
    "    # index 3 stands for an unknown token\n",
    "    if emoji_tensor.size(1) == 3 and emoji_vocab_idx != 3:\n",
    "        emojis_in_tokenizer[em] = emoji_vocab_idx\n",
    "print(f\"{len(emojis_in_tokenizer)} of our emojis are in this model\")\n",
    "\n",
    "emojis_not_in_tokenizer = set(all_emojis) - set(emojis_in_tokenizer.keys())\n",
    "original_tokenizer_size = len(tokenizer)\n",
    "print(f\"Original number of tokens: {original_tokenizer_size}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",\\\n",
    "                                         additional_special_tokens=list(emojis_not_in_tokenizer))\n",
    "print(f\"Number of tokens after extension: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_emojis = pd.read_csv(AMBIGUITY_CLUSTER).emoji.unique()\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "# emojis_in_tokenizer = {}\n",
    "# for em in all_emojis:\n",
    "#     emoji_tensor = tokenizer(emoji.demojize(em), return_tensors='pt')['input_ids']\n",
    "#     emoji_vocab_idx = emoji_tensor[0][1].item()\n",
    "#     # if size is 3 it means emoji token was not splitted so it is a known token,\n",
    "#     # [start] [emoji] [stop]\n",
    "#     # index 3 stands for an unknown token\n",
    "#     if emoji_tensor.size(1) == 3 and emoji_vocab_idx != 3:\n",
    "#         emojis_in_tokenizer[em] = emoji_vocab_idx\n",
    "\n",
    "# print(f\"{len(emojis_in_tokenizer)} of our emojis are in this model\")\n",
    "# del all_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "middle-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis_in_tokenizer_indices = set(emojis_in_tokenizer.values())\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def get_emoji_embedding(text):\n",
    "    tokenized = np.array(tokenizer.tokenize(text))\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    tokens_ids = encoded_input['input_ids']\n",
    "    mask = [id_.item() in emojis_in_tokenizer_indices \\\n",
    "            or id_.item() >= original_tokenizer_size for id_ in tokens_ids[0]]\n",
    "    if any(mask):\n",
    "        try:\n",
    "            features = model(**encoded_input)[0]\n",
    "        except IndexError:\n",
    "            return np.nan, np.nan\n",
    "        return features[0][mask][:].detach().numpy(), tokenized[mask[1:-1]]\n",
    "    else:\n",
    "        return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tutorial-anderson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4fc18b230a4940ac20b527cbb70152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (188 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "out = tweets.progress_apply(get_emoji_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "herbal-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "southwest-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[[\"embedding\", \"emoji\"]] = pd.DataFrame(out.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "global-model",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>embedding</th>\n",
       "      <th>emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>:keycap_asterisk:  :SOS_button: PLEASE    :SO...</td>\n",
       "      <td>[[-0.3886098, 0.19909568, 0.20366114, 0.119760...</td>\n",
       "      <td>[:SOS_button:, :SOS_button:, :pouting_face:, :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:keycap_asterisk: PLEASE   KILL RATE :warning...</td>\n",
       "      <td>[[-0.18006775, 0.1269213, 0.13839212, 0.054697...</td>\n",
       "      <td>[:warning_selector:, :warning_selector:]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:keycap_asterisk:  :diamond_with_a_dot:  :key...</td>\n",
       "      <td>[[0.45944917, -0.12830271, 0.1478465, 0.080913...</td>\n",
       "      <td>[:diamond_with_a_dot:, :diamond_with_a_dot:, :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"SAFFRON\"  :orange_heart: Sweet male puppy 3 m...</td>\n",
       "      <td>[[0.13286656, -0.041414626, 0.34103054, 0.1911...</td>\n",
       "      <td>[:orange_heart:, :green_heart:]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:keycap_asterisk: PLEASE   3 DUMPED IN DROP B...</td>\n",
       "      <td>[[-0.42245123, -0.23179615, -0.060957894, -0.1...</td>\n",
       "      <td>[:pouting_face:, :syringe:]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0   :keycap_asterisk:  :SOS_button: PLEASE    :SO...   \n",
       "1   :keycap_asterisk: PLEASE   KILL RATE :warning...   \n",
       "2   :keycap_asterisk:  :diamond_with_a_dot:  :key...   \n",
       "3  \"SAFFRON\"  :orange_heart: Sweet male puppy 3 m...   \n",
       "4   :keycap_asterisk: PLEASE   3 DUMPED IN DROP B...   \n",
       "\n",
       "                                           embedding  \\\n",
       "0  [[-0.3886098, 0.19909568, 0.20366114, 0.119760...   \n",
       "1  [[-0.18006775, 0.1269213, 0.13839212, 0.054697...   \n",
       "2  [[0.45944917, -0.12830271, 0.1478465, 0.080913...   \n",
       "3  [[0.13286656, -0.041414626, 0.34103054, 0.1911...   \n",
       "4  [[-0.42245123, -0.23179615, -0.060957894, -0.1...   \n",
       "\n",
       "                                               emoji  \n",
       "0  [:SOS_button:, :SOS_button:, :pouting_face:, :...  \n",
       "1           [:warning_selector:, :warning_selector:]  \n",
       "2  [:diamond_with_a_dot:, :diamond_with_a_dot:, :...  \n",
       "3                    [:orange_heart:, :green_heart:]  \n",
       "4                        [:pouting_face:, :syringe:]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "double-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "surrounded-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"embedding\"] = tweets[\"embedding\"].parallel_apply(lambda x: x.tolist())\n",
    "tweets[\"emoji\"] = tweets[\"emoji\"].parallel_apply(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acute-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.set_index(['tweet']).apply(pd.Series.explode).reset_index()\n",
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "naval-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(tweets, \"/scratch/czestoch/bert_emojis_with_unknown_emojis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "french-trial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emoji\n",
       ":1st_place_medal:            467\n",
       ":2nd_place_medal:            154\n",
       ":3rd_place_medal:            130\n",
       ":AB_button_(blood_type):     116\n",
       ":ATM_sign:                    63\n",
       "                            ... \n",
       ":zany_face:                 1135\n",
       ":zebra:                      225\n",
       ":zipper-mouth_face:           98\n",
       ":zombie:                      47\n",
       ":zzz:                        272\n",
       "Name: embedding, Length: 1193, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupby(\"emoji\").embedding.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-polyester",
   "metadata": {},
   "source": [
    "## Check if unknown tokens are rubbish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "italic-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    features = model(**encoded_input)\n",
    "    return features[1].detach().cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "least-coverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(64003, 768)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",\\\n",
    "                                         additional_special_tokens=[\":paintbrush_selector:\",\\\n",
    "                                                                    emoji.demojize('❤️')])\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "sized-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors are the same: False\n",
      "Difference between vectors: -0.24916860461235046\n"
     ]
    }
   ],
   "source": [
    "# print(f\"This is an initially known token: {emoji.demojize('🖌️') in emojis_in_tokenizer}\")\n",
    "# print(f\"This is an initially known token: {emoji.demojize('❤️') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! 🖌️\"\n",
    "text2 = \"This is amazing, trust me! ❤️\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "print(f\"Vectors are the same: {(one_pass == second_pass).all()}\")\n",
    "print(f\"Difference between vectors: {(one_pass - second_pass).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "digital-berkeley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: False\n",
      "This is an initially known token: False\n",
      "Vectors are the same: False\n",
      "Difference between vectors: 1.4294824600219727\n"
     ]
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('🖌️') in emojis_in_tokenizer}\")\n",
    "print(f\"This is an initially known token: {emoji.demojize('❤️') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! 🖌️\"\n",
    "text2 = \"This is amazing, trust me! ❤️\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "print(f\"Vectors are the same: {(one_pass == second_pass).all()}\")\n",
    "print(f\"Difference between vectors: {(one_pass - second_pass).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "divine-peeing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: True\n",
      "This is an initially known token: True\n",
      "Vectors are the same: False\n",
      "Difference between vectors: -0.09044761955738068\n"
     ]
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('😂') in emojis_in_tokenizer}\")\n",
    "print(f\"This is an initially known token: {emoji.demojize('💓') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! 😂\"\n",
    "text2 = \"This is amazing, trust me! 💓\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "print(f\"Vectors are the same: {(one_pass == second_pass).all()}\")\n",
    "print(f\"Difference between vectors: {(one_pass - second_pass).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "consistent-terminology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(f\"This is an initially known token: {emoji.demojize('🖌️') in emojis_in_tokenizer}\")\n",
    "# print(f\"This is an initially known token: {emoji.demojize('❤️') in emojis_in_tokenizer}\")\n",
    "text2 = \"This is amazing, trust me! 🖌️\"\n",
    "text2 = emoji.demojize(text2)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass == second_pass).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "hollywood-adventure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.47473258"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('🖌️') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! 🖌️\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it 🖌️\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Heart is initially not in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "thick-minority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.48091918"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('❤️') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! ❤️\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it ❤️\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Emojis below are in the original vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "difficult-grill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.2963187"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('😂') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! 😂\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it 😂\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "renewable-sherman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.20267165"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('💓') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! 💓\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it 💓\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "applicable-modern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.60374236"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('🧡') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! 🧡\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it 🧡\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check if embeddinga on unknown tokens that were added to vocabulary are rubish\n",
    "### check which emojis are in the tokenizer and how to extract their embeddings later\n",
    "### check parallelization\n",
    "### save embeddings, yupi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
