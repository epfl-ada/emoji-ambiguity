{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "breeding-driving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=4)\n",
    "tqdm.pandas()\n",
    "\n",
    "from settings import AMBIGUITY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rough-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/jczestochowska/workspace/dlab/emoji-ambiguity/data/interim/sampled_tweets.txt.gz\"\n",
    "tweets = pd.read_csv(path, header=0, lineterminator='\\n', encoding='utf-8')['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "apparent-founder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93390"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "magnetic-hunger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    üß°\"STAR\" Adorable female puppy 3 months old *‚É£i...\n",
       "1       *‚É£PLEASE   GUNNER üíâ4a COLD, NEEDS HERO&amp;üè°*‚É£\n",
       "2    üÜòMAS SHELTER AT CAPACITYüíâüö® *‚É£PLEASE   PLS HELP...\n",
       "3    RT / REPLY to VOTE! *‚É£  *‚É£  *‚É£ Not much sweat,...\n",
       "4    *‚É£üÜòPLEASE   üÜòWE WILL NOT BE SILENTüò°plz watchüìº ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "finnish-theta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets[:1000]\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "functioning-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(text):\n",
    "    return add_spaces_between_emojis(emoji.demojize(text))\n",
    "\n",
    "def add_spaces_between_emojis(demojified_text):\n",
    "    new_text = []\n",
    "    colons = []\n",
    "    for char in demojified_text:\n",
    "        if char == \":\":\n",
    "            if colons:\n",
    "                new_text.append(char + \" \")\n",
    "                colons.pop()\n",
    "            else:\n",
    "                colons.append(char)\n",
    "                new_text.append(\" \" + char)\n",
    "        else:\n",
    "            new_text.append(char)\n",
    "    return ''.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "clean-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.parallel_apply(preprocess_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dated-drill",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 of our emojis are in this model\n"
     ]
    }
   ],
   "source": [
    "all_emojis = pd.read_csv(AMBIGUITY_PATH).emoji.unique()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "emojis_in_tokenizer = {}\n",
    "for em in all_emojis:\n",
    "    emoji_tensor = tokenizer(emoji.demojize(em), return_tensors='pt')['input_ids']\n",
    "    emoji_vocab_idx = emoji_tensor[0][1].item()\n",
    "    # if size is 3 it means emoji token was not splitted so it is a known token,\n",
    "    # [start] [emoji] [stop]\n",
    "    # index 3 stands for an unknown token\n",
    "    if emoji_tensor.size(1) == 3 and emoji_vocab_idx != 3:\n",
    "        emojis_in_tokenizer[em] = emoji_vocab_idx\n",
    "\n",
    "print(f\"{len(emojis_in_tokenizer)} of our emojis are in this model\")\n",
    "del all_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "domestic-brunswick",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "emojis_in_tokenizer_indices = set(emojis_in_tokenizer.values())\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "def get_emoji_embedding(text):\n",
    "    tokenized = np.array(tokenizer.tokenize(text))\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    tokens_ids = encoded_input['input_ids']\n",
    "    mask = [id_.item() in emojis_in_tokenizer_indices for id_ in tokens_ids[0]]\n",
    "    if any(mask):\n",
    "        try:\n",
    "            features = model(**encoded_input)[0]\n",
    "        except IndexError:\n",
    "            return np.nan, np.nan\n",
    "        return features[0][mask][:].detach().numpy(), tokenized[mask[1:-1]]\n",
    "    else:\n",
    "        return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "restricted-green",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2490b709c711490bacacbdc3c77c6a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (140 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out = tweets.progress_apply(get_emoji_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "parental-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "banned-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[[\"embedding\", \"emoji\"]] = pd.DataFrame(out.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-fourth",
   "metadata": {},
   "source": [
    "## Check if unknown tokens are rubbish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    features = model(**encoded_input)\n",
    "    return features[0].detach().cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",\\\n",
    "                                         additional_special_tokens=[\":paintbrush_selector:\"])\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paintbrush is initially not in the vocabulary but was added as an additional token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This is an initially known token: {'üñåÔ∏è' in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üñåÔ∏è\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it üñåÔ∏è\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Heart is initially not in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This is an initially known token: {'‚ù§Ô∏è' in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! ‚ù§Ô∏è\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it ‚ù§Ô∏è\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Emojis below are in the original vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This is an initially known token: {'üòÇ' in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üòÇ\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it üòÇ\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This is an initially known token: {'üíì' in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üíì\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it üíì\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This is an initially known token: {'üß°' in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üß°\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it üß°\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check if embeddinga on unknown tokens that were added to vocabulary are rubish\n",
    "### check which emojis are in the tokenizer and how to extract their embeddings later\n",
    "### check parallelization\n",
    "### save embeddings, yupi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
