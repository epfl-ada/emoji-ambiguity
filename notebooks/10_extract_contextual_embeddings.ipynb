{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czestoch/.local/lib/python3.7/site-packages/tqdm/std.py:702: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=8)\n",
    "tqdm.pandas()\n",
    "\n",
    "from settings import AMBIGUITY_PATH, AMBIGUITY_CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.utils import save_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/scratch/czestoch/sampled_tweets.txt.gz\"\n",
    "tweets = pd.read_csv(path, header=0, lineterminator='\\n', encoding='utf-8')['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93390"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    *‚É£üÜòPLEASE   üÜòWE WILL NOT BE SILENTüò°plz watchüìº ...\n",
       "1       *‚É£PLEASE   KILL RATE‚ö†Ô∏èRESCUE ONLY‚ö†Ô∏èThank you*‚É£\n",
       "2    *‚É£üí†*‚É£üí†*‚É£üí†*‚É£üí†*‚É£üí†*‚É£üí† How to TRIGGER a LIBERAL wi...\n",
       "3    \"SAFFRON\" üß°Sweet male puppy 3 months 11.5 lbs ...\n",
       "4          *‚É£PLEASE   3 DUMPED IN DROP BOXüò°DIESüíâ2/22*‚É£\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets = tweets[:1000]\n",
    "# len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(text):\n",
    "    return add_spaces_between_emojis(emoji.demojize(text))\n",
    "\n",
    "def add_spaces_between_emojis(demojified_text):\n",
    "    new_text = []\n",
    "    colons = []\n",
    "    for char in demojified_text:\n",
    "        if char == \":\":\n",
    "            if colons:\n",
    "                new_text.append(char + \" \")\n",
    "                colons.pop()\n",
    "            else:\n",
    "                colons.append(char)\n",
    "                new_text.append(\" \" + char)\n",
    "        else:\n",
    "            new_text.append(char)\n",
    "    return ''.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.parallel_apply(preprocess_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e8278406ad4107b441beb88ac7fa9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f932f11da9cd46c9aec040f5df7f1f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/843k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af96a6c02864896a8930cba2fc0f4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 of our emojis are in this model\n"
     ]
    }
   ],
   "source": [
    "all_emojis = pd.read_csv(AMBIGUITY_CLUSTER).emoji.unique()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "emojis_in_tokenizer = {}\n",
    "for em in all_emojis:\n",
    "    emoji_tensor = tokenizer(emoji.demojize(em), return_tensors='pt')['input_ids']\n",
    "    emoji_vocab_idx = emoji_tensor[0][1].item()\n",
    "    # if size is 3 it means emoji token was not splitted so it is a known token,\n",
    "    # [start] [emoji] [stop]\n",
    "    # index 3 stands for an unknown token\n",
    "    if emoji_tensor.size(1) == 3 and emoji_vocab_idx != 3:\n",
    "        emojis_in_tokenizer[em] = emoji_vocab_idx\n",
    "\n",
    "print(f\"{len(emojis_in_tokenizer)} of our emojis are in this model\")\n",
    "del all_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9505bb029be4a4ba772d658e4a3e3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "emojis_in_tokenizer_indices = set(emojis_in_tokenizer.values())\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "def get_emoji_embedding(text):\n",
    "    tokenized = np.array(tokenizer.tokenize(text))\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    tokens_ids = encoded_input['input_ids']\n",
    "    mask = [id_.item() in emojis_in_tokenizer_indices for id_ in tokens_ids[0]]\n",
    "    if any(mask):\n",
    "        try:\n",
    "            features = model(**encoded_input)[0]\n",
    "        except IndexError:\n",
    "            return np.nan, np.nan\n",
    "        return features[0][mask][:].detach().numpy(), tokenized[mask[1:-1]]\n",
    "    else:\n",
    "        return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae76de6d9cb4a3e8b7a38e3c495ce1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (188 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "out = tweets.progress_apply(get_emoji_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[[\"embedding\", \"emoji\"]] = pd.DataFrame(out.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>embedding</th>\n",
       "      <th>emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>:keycap_asterisk:  :SOS_button: PLEASE    :SO...</td>\n",
       "      <td>[[-0.40534008, 0.29607773, 0.18809724, 0.15452...</td>\n",
       "      <td>[:SOS_button:, :SOS_button:, :pouting_face:]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:keycap_asterisk: PLEASE   KILL RATE :warning...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:keycap_asterisk:  :diamond_with_a_dot:  :key...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"SAFFRON\"  :orange_heart: Sweet male puppy 3 m...</td>\n",
       "      <td>[[0.13286656, -0.041414626, 0.34103054, 0.1911...</td>\n",
       "      <td>[:orange_heart:, :green_heart:]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:keycap_asterisk: PLEASE   3 DUMPED IN DROP B...</td>\n",
       "      <td>[[-0.42245123, -0.23179615, -0.060957894, -0.1...</td>\n",
       "      <td>[:pouting_face:, :syringe:]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0   :keycap_asterisk:  :SOS_button: PLEASE    :SO...   \n",
       "1   :keycap_asterisk: PLEASE   KILL RATE :warning...   \n",
       "2   :keycap_asterisk:  :diamond_with_a_dot:  :key...   \n",
       "3  \"SAFFRON\"  :orange_heart: Sweet male puppy 3 m...   \n",
       "4   :keycap_asterisk: PLEASE   3 DUMPED IN DROP B...   \n",
       "\n",
       "                                           embedding  \\\n",
       "0  [[-0.40534008, 0.29607773, 0.18809724, 0.15452...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  [[0.13286656, -0.041414626, 0.34103054, 0.1911...   \n",
       "4  [[-0.42245123, -0.23179615, -0.060957894, -0.1...   \n",
       "\n",
       "                                          emoji  \n",
       "0  [:SOS_button:, :SOS_button:, :pouting_face:]  \n",
       "1                                           NaN  \n",
       "2                                           NaN  \n",
       "3               [:orange_heart:, :green_heart:]  \n",
       "4                   [:pouting_face:, :syringe:]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(tweets, \"/scratch/czestoch/bert_emojis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"embedding\"] = tweets[\"embedding\"].parallel_apply(lambda x: x.tolist())\n",
    "tweets[\"emoji\"] = tweets[\"emoji\"].parallel_apply(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.set_index(['tweet']).apply(pd.Series.explode).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(tweets, \"/scratch/czestoch/bert_emojis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emoji\n",
       ":COOL_button:           115\n",
       ":Christmas_tree:        353\n",
       ":FREE_button:           194\n",
       ":OK_button:             196\n",
       ":OK_hand:               599\n",
       "                       ... \n",
       ":wrapped_gift:          586\n",
       ":yellow_heart:          935\n",
       ":zany_face:            1116\n",
       ":zipper-mouth_face:      94\n",
       ":zzz:                   256\n",
       "Name: embedding, Length: 437, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupby(\"emoji\").embedding.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if unknown tokens are rubbish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    features = model(**encoded_input)\n",
    "    return features[0].detach().cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",\\\n",
    "                                         additional_special_tokens=[\":paintbrush_selector:\"])\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paintbrush is initially not in the vocabulary but was added as an additional token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This is an initially known token: {'üñåÔ∏è' in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üñåÔ∏è\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it üñåÔ∏è\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Heart is initially not in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This is an initially known token: {'‚ù§Ô∏è' in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! ‚ù§Ô∏è\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it ‚ù§Ô∏è\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Emojis below are in the original vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This is an initially known token: {'üòÇ' in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üòÇ\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it üòÇ\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This is an initially known token: {'üíì' in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üíì\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it üíì\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This is an initially known token: {'üß°' in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üß°\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it üß°\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check if embeddinga on unknown tokens that were added to vocabulary are rubish\n",
    "### check which emojis are in the tokenizer and how to extract their embeddings later\n",
    "### check parallelization\n",
    "### save embeddings, yupi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
