{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czestoch/.local/lib/python3.7/site-packages/tqdm/std.py:702: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from emoji import demojize\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=8)\n",
    "tqdm.pandas()\n",
    "\n",
    "from settings import AMBIGUITY_PATH, AMBIGUITY_CLUSTER\n",
    "\n",
    "from src.data.utils import save_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Load data...\")\n",
    "path = \"/scratch/czestoch/sampled_tweets_bigger.txt.gz\"\n",
    "tweets = pd.read_csv(path, header=0, lineterminator='\\n', encoding='utf-8')\n",
    "# tweets.emojis = tweets.emojis.parallel_apply(eval)\n",
    "\n",
    "# tweets = tweets.explode(\"emojis\", ignore_index=True)\n",
    "# tweets = tweets.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90' - 3 minuti di recupero  0Ô∏è‚É£ - 3Ô∏è‚É£</td>\n",
       "      <td>0Ô∏è‚É£</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ AC MILAN 0Ô∏è‚É£-1Ô∏è‚É£ BENEVENTO ‚è±FIN DU MATCH !</td>\n",
       "      <td>0Ô∏è‚É£</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0Ô∏è‚É£„Ä∞Ô∏èüíØ real quick</td>\n",
       "      <td>0Ô∏è‚É£</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ronaldo's last away goal in La Liga? October 1...</td>\n",
       "      <td>0Ô∏è‚É£</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(L1) ANGERS 0Ô∏è‚É£-2Ô∏è‚É£ LORIENT (L2) (N2) GRANVILL...</td>\n",
       "      <td>0Ô∏è‚É£</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet emojis\n",
       "0              90' - 3 minuti di recupero  0Ô∏è‚É£ - 3Ô∏è‚É£    0Ô∏è‚É£\n",
       "1       [ AC MILAN 0Ô∏è‚É£-1Ô∏è‚É£ BENEVENTO ‚è±FIN DU MATCH !    0Ô∏è‚É£\n",
       "2                                  0Ô∏è‚É£„Ä∞Ô∏èüíØ real quick    0Ô∏è‚É£\n",
       "3  Ronaldo's last away goal in La Liga? October 1...    0Ô∏è‚É£\n",
       "4  (L1) ANGERS 0Ô∏è‚É£-2Ô∏è‚É£ LORIENT (L2) (N2) GRANVILL...    0Ô∏è‚É£"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163900"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_emojis = pd.read_csv(AMBIGUITY_CLUSTER, encoding='utf-8').emoji.unique()\n",
    "tweets = tweets[tweets.emojis.isin(our_emojis)]\n",
    "df = tweets.groupby(\"emojis\").count()\n",
    "numerous_emojis = df[df.tweet >= 100].index.tolist()\n",
    "tweets = tweets[tweets.emojis.isin(numerous_emojis)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess tweets...\n",
      "Load model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bb7b1cd06845fa90ab6586fd28f058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "our_emojis = pd.read_csv(AMBIGUITY_CLUSTER, encoding='utf-8').emoji.unique()\n",
    "tweets = tweets[tweets.emojis.isin(our_emojis)]\n",
    "\n",
    "df = tweets.groupby(\"emojis\").count()\n",
    "numerous_emojis = df[df.tweet >= 100].index.tolist()\n",
    "tweets = tweets[tweets.emojis.isin(numerous_emojis)]\n",
    "\n",
    "tweets = tweets[:1000]\n",
    "\n",
    "del our_emojis\n",
    "del numerous_emojis\n",
    "del df\n",
    "\n",
    "def preprocess_tweets(group):\n",
    "    emoji = group.emojis.unique()[0]\n",
    "    if emoji == \"*‚É£\" or emoji == '*Ô∏è‚É£':\n",
    "        emoji = f\"\\{emoji}\"\n",
    "    try:\n",
    "        group.tweet = group.tweet.apply(lambda x: x.replace(emoji, \"<mask>\", 1))\n",
    "        group.tweet = group.tweet.apply(demojize)\n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "    return group\n",
    "\n",
    "def get_emoji_softmax_variance(texts):\n",
    "    try:\n",
    "        tokenized = [tokenizer.tokenize(text) for text in texts.tolist()]\n",
    "        tokenized = list(filter(lambda x: \"<mask>\" in x, tokenized))\n",
    "        input_ = tokenizer(tokenized, return_tensors='pt', is_split_into_words=True, padding=True, truncation=True)\n",
    "        mask_index = torch.where(input_[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        output = model(**input_)\n",
    "        logits = output.logits\n",
    "        softmax = F.softmax(logits, dim=-1)\n",
    "        over_our_words = softmax[torch.arange(softmax.size(0)), mask_index][:, indices].detach().numpy()\n",
    "        return np.sum(np.var(over_our_words, 0))\n",
    "#         return softmax[torch.arange(softmax.size(0)), mask_index].detach().numpy().tolist()\n",
    "#         return softmax[0, mask_index[0], :].detach().numpy().tolist()\n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "print(\"Preprocess tweets...\")\n",
    "tweets = tweets.groupby(\"emojis\").parallel_apply(preprocess_tweets)\n",
    "tweets = tweets.dropna()\n",
    "\n",
    "print(\"Load model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "# get indices of emoji words from out data in bert vocabulary\n",
    "our_words = set(pd.read_csv(AMBIGUITY_CLUSTER).word.unique())\n",
    "indices = []\n",
    "for vocab_idx, vocab_word in enumerate(vocab):\n",
    "    if vocab_word in our_words:\n",
    "        indices.append(vocab_idx)\n",
    "indices = np.array(indices)\n",
    "\n",
    "print(\"Extracting embeddings...\")\n",
    "out = tweets.groupby(\"emojis\").tweet.progress_apply(get_emoji_softmax_variance)\n",
    "del tweets\n",
    "out = out.dropna()\n",
    "out = out.reset_index()\n",
    "\n",
    "print(\"Saving...\")\n",
    "# save_to_csv(out, \"/scratch/czestoch/softmax_emojis_variances.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emojis</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¬©Ô∏è</td>\n",
       "      <td>0.003143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¬ÆÔ∏è</td>\n",
       "      <td>0.007534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‚ÄºÔ∏è</td>\n",
       "      <td>0.006471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‚ÅâÔ∏è</td>\n",
       "      <td>0.005759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‚Ñ¢Ô∏è</td>\n",
       "      <td>0.020594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>‚ÑπÔ∏è</td>\n",
       "      <td>0.007747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>‚ÜîÔ∏è</td>\n",
       "      <td>0.013571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‚ÜóÔ∏è</td>\n",
       "      <td>0.007061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>‚ÜòÔ∏è</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emojis     tweet\n",
       "0     ¬©Ô∏è  0.003143\n",
       "1     ¬ÆÔ∏è  0.007534\n",
       "2     ‚ÄºÔ∏è  0.006471\n",
       "3     ‚ÅâÔ∏è  0.005759\n",
       "4     ‚Ñ¢Ô∏è  0.020594\n",
       "5     ‚ÑπÔ∏è  0.007747\n",
       "6     ‚ÜîÔ∏è  0.013571\n",
       "7     ‚ÜóÔ∏è  0.007061\n",
       "8     ‚ÜòÔ∏è  0.000491"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czestoch/.local/lib/python3.7/site-packages/tqdm/std.py:702: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Preprocess tweets...\n",
      "Load model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings...\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from emoji import demojize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=8)\n",
    "tqdm.pandas()\n",
    "\n",
    "from settings import AMBIGUITY_CLUSTER\n",
    "\n",
    "from src.data.utils import save_to_csv\n",
    "\n",
    "\n",
    "def preprocess_tweets(group):\n",
    "    emoji = group.emojis.unique()[0]\n",
    "    if emoji == \"*‚É£\" or emoji == '*Ô∏è‚É£':\n",
    "        emoji = f\"\\{emoji}\"\n",
    "    try:\n",
    "        group.tweet = group.tweet.apply(lambda x: x.replace(emoji, \"[EMOJI]\", 1))\n",
    "        group.tweet = group.tweet.apply(demojize)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return group\n",
    "\n",
    "def get_embeddings_variance(group):\n",
    "    try:\n",
    "        encoded_input = tokenizer(group.tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "        embeddings = model(**encoded_input)[1][0].detach().numpy()\n",
    "        return np.sum(embeddings.var(0))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "    \n",
    "print(\"Load data...\")\n",
    "path = \"/scratch/czestoch/sampled_tweets_bigger.txt.gz\"\n",
    "tweets = pd.read_csv(path, header=0, lineterminator='\\n', encoding='utf-8')\n",
    "tweets = tweets[:1000]\n",
    "\n",
    "print(\"Preprocess tweets...\")\n",
    "tweets = tweets.groupby(\"emojis\").parallel_apply(preprocess_tweets)\n",
    "tweets = tweets.dropna()\n",
    "\n",
    "print(\"Load model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",\\\n",
    "                                        additional_special_tokens=[\"[EMOJI]\"])\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "vocab = list(tokenizer.encoder.keys())\n",
    "\n",
    "print(\"Extracting embeddings...\")\n",
    "# variances = tweets.groupby(\"emojis\").tweet.progress_apply(get_embeddings_variance)\n",
    "# variances = variances.dropna()\n",
    "# variances = variances.reset_index().rename({0: \"variance\"}, axis=1)\n",
    "\n",
    "# save_to_csv(variances, \"/scratch/czestoch/emojis_masked_variances.csv\")\n",
    "# save_to_csv(variances, args.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_words = set(pd.read_csv(AMBIGUITY_CLUSTER).word.unique())\n",
    "indices = []\n",
    "for vocab_idx, vocab_word in enumerate(vocab):\n",
    "    if vocab_word in our_words:\n",
    "        indices.append(vocab_idx)\n",
    "indices = np.array(indices)\n",
    "\n",
    "group = tweets[tweets.emojis == '0Ô∏è‚É£']\n",
    "group = group.tweet\n",
    "encoded_input = tokenizer(group.tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "embeddings = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 128, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess tweets...\n",
      "Load model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98382f38d024e0c8efa0393837d08b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# explode\n",
    "# groupby emoji\n",
    "# mask emoji\n",
    "# extract tweet embedding\n",
    "\n",
    "tweets1 = tweets[:1000]\n",
    "\n",
    "def preprocess_tweets(group):\n",
    "    emoji = group.emojis.unique()[0]\n",
    "    if emoji == \"*‚É£\" or emoji == '*Ô∏è‚É£':\n",
    "        emoji = f\"\\{emoji}\"\n",
    "    try:\n",
    "        group.tweet = group.tweet.replace(emoji, \"[EMOJI]\", regex=True)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return group\n",
    "\n",
    "def get_tweet_embedding(text):\n",
    "    try:\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        return model(**encoded_input)[1][0].detach().numpy().tolist()\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "print(\"Preprocess tweets...\")\n",
    "tweets1 = tweets1.groupby(\"emojis\").parallel_apply(preprocess_tweets)\n",
    "tweets1 = tweets1.dropna()\n",
    "\n",
    "print(\"Load model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",\\\n",
    "                                         additional_special_tokens=[\"[EMOJI]\"])\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(\"Extracting embeddings...\")\n",
    "out = tweets1.tweet.progress_apply(get_tweet_embedding)\n",
    "\n",
    "tweets1[\"embedding\"] = out\n",
    "tweets1 = tweets1.dropna()\n",
    "# save_to_csv(tweets, \"/scratch/czestoch/bert_emojis_masked.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>emojis</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90' - 3 minuti di recupero  [EMOJI] - 3Ô∏è‚É£</td>\n",
       "      <td>0Ô∏è‚É£</td>\n",
       "      <td>[0.2714495360851288, -0.17950321733951569, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ AC MILAN [EMOJI]-1Ô∏è‚É£ BENEVENTO ‚è±FIN DU MATCH !</td>\n",
       "      <td>0Ô∏è‚É£</td>\n",
       "      <td>[0.21306820213794708, -0.20475709438323975, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[EMOJI]„Ä∞Ô∏èüíØ real quick</td>\n",
       "      <td>0Ô∏è‚É£</td>\n",
       "      <td>[0.30385464429855347, -0.21020273864269257, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ronaldo's last away goal in La Liga? October 1...</td>\n",
       "      <td>0Ô∏è‚É£</td>\n",
       "      <td>[0.2292073518037796, -0.15393587946891785, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(L1) ANGERS [EMOJI]-2Ô∏è‚É£ LORIENT (L2) (N2) GRAN...</td>\n",
       "      <td>0Ô∏è‚É£</td>\n",
       "      <td>[0.21869970858097076, -0.05074208974838257, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[EMOJI]0‚É£ for Dhawan who, along with Manish Pa...</td>\n",
       "      <td>5‚É£</td>\n",
       "      <td>[0.19394651055335999, -0.13764727115631104, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Feliz [EMOJI][EMOJI], GOAT! üêê</td>\n",
       "      <td>5‚É£</td>\n",
       "      <td>[0.31594470143318176, -0.17039579153060913, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[EMOJI]0‚É£ apperances for Joel Matip today. üëè</td>\n",
       "      <td>5‚É£</td>\n",
       "      <td>[0.2831932604312897, -0.17760756611824036, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The Final NCAA stats released &amp;amp; our own fi...</td>\n",
       "      <td>5‚É£</td>\n",
       "      <td>[0.18135130405426025, -0.12419982254505157, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[EMOJI] mins to go! We keep battling... Come o...</td>\n",
       "      <td>5‚É£</td>\n",
       "      <td>[0.18057197332382202, -0.10110711306333542, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet emojis  \\\n",
       "0            90' - 3 minuti di recupero  [EMOJI] - 3Ô∏è‚É£    0Ô∏è‚É£   \n",
       "1     [ AC MILAN [EMOJI]-1Ô∏è‚É£ BENEVENTO ‚è±FIN DU MATCH !    0Ô∏è‚É£   \n",
       "2                                [EMOJI]„Ä∞Ô∏èüíØ real quick    0Ô∏è‚É£   \n",
       "3    Ronaldo's last away goal in La Liga? October 1...    0Ô∏è‚É£   \n",
       "4    (L1) ANGERS [EMOJI]-2Ô∏è‚É£ LORIENT (L2) (N2) GRAN...    0Ô∏è‚É£   \n",
       "..                                                 ...    ...   \n",
       "995  [EMOJI]0‚É£ for Dhawan who, along with Manish Pa...     5‚É£   \n",
       "996                      Feliz [EMOJI][EMOJI], GOAT! üêê     5‚É£   \n",
       "997       [EMOJI]0‚É£ apperances for Joel Matip today. üëè     5‚É£   \n",
       "998  The Final NCAA stats released &amp; our own fi...     5‚É£   \n",
       "999  [EMOJI] mins to go! We keep battling... Come o...     5‚É£   \n",
       "\n",
       "                                             embedding  \n",
       "0    [0.2714495360851288, -0.17950321733951569, 0.0...  \n",
       "1    [0.21306820213794708, -0.20475709438323975, 0....  \n",
       "2    [0.30385464429855347, -0.21020273864269257, 0....  \n",
       "3    [0.2292073518037796, -0.15393587946891785, 0.0...  \n",
       "4    [0.21869970858097076, -0.05074208974838257, -0...  \n",
       "..                                                 ...  \n",
       "995  [0.19394651055335999, -0.13764727115631104, 0....  \n",
       "996  [0.31594470143318176, -0.17039579153060913, 0....  \n",
       "997  [0.2831932604312897, -0.17760756611824036, 0.0...  \n",
       "998  [0.18135130405426025, -0.12419982254505157, 0....  \n",
       "999  [0.18057197332382202, -0.10110711306333542, 0....  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.373917717602736"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(tweets1[tweets1.emojis == '0Ô∏è‚É£'].embedding.values.tolist()).var(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance(group):\n",
    "    return np.sum(np.array(group.embedding.values.tolist()).var(0))\n",
    "\n",
    "variances = tweets1.groupby(\"emojis\").parallel_apply(calculate_variance)\n",
    "variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# print(\"Extract embeddings...\")\n",
    "# out, i = [], 0\n",
    "# for _, text in tweets.tweet.iteritems():\n",
    "#     out.append(get_tweet_embedding(text))\n",
    "    \n",
    "#     if i % 500 == 0:\n",
    "#         print(\"checkpoint: %s\" % i)\n",
    "#         gc.collect()\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0,1,2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>emojis</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[EMOJI]üÜòPLEASE   üÜòWE WILL NOT BE SILENTüò°plz wa...</td>\n",
       "      <td>*‚É£</td>\n",
       "      <td>[0.3089597523212433, -0.18129678070545197, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*‚É£[EMOJI]PLEASE   [EMOJI]WE WILL NOT BE SILENT...</td>\n",
       "      <td>üÜò</td>\n",
       "      <td>[0.28499189019203186, -0.13812287151813507, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*‚É£üÜòPLEASE   üÜòWE WILL NOT BE SILENT[EMOJI]plz w...</td>\n",
       "      <td>üò°</td>\n",
       "      <td>[0.3065902590751648, -0.15257291495800018, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*‚É£üÜòPLEASE   üÜòWE WILL NOT BE SILENTüò°plz watch[E...</td>\n",
       "      <td>üìº</td>\n",
       "      <td>[0.31592175364494324, -0.18171299993991852, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[EMOJI]PLEASE   KILL RATE‚ö†Ô∏èRESCUE ONLY‚ö†Ô∏èThank ...</td>\n",
       "      <td>*‚É£</td>\n",
       "      <td>[0.25296467542648315, -0.2111043781042099, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet emojis  \\\n",
       "0  [EMOJI]üÜòPLEASE   üÜòWE WILL NOT BE SILENTüò°plz wa...     *‚É£   \n",
       "1  *‚É£[EMOJI]PLEASE   [EMOJI]WE WILL NOT BE SILENT...      üÜò   \n",
       "2  *‚É£üÜòPLEASE   üÜòWE WILL NOT BE SILENT[EMOJI]plz w...      üò°   \n",
       "3  *‚É£üÜòPLEASE   üÜòWE WILL NOT BE SILENTüò°plz watch[E...      üìº   \n",
       "4  [EMOJI]PLEASE   KILL RATE‚ö†Ô∏èRESCUE ONLY‚ö†Ô∏èThank ...     *‚É£   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.3089597523212433, -0.18129678070545197, 0.0...  \n",
       "1  [0.28499189019203186, -0.13812287151813507, -0...  \n",
       "2  [0.3065902590751648, -0.15257291495800018, -0....  \n",
       "3  [0.31592175364494324, -0.18171299993991852, -0...  \n",
       "4  [0.25296467542648315, -0.2111043781042099, 0.0...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"/scratch/czestoch/bert_emojis_masked.csv.gz\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(test.dropna(), \"/scratch/czestoch/bert_emojis_masked.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in test.iterrows():\n",
    "    try:\n",
    "        assert len(eval(row.embedding)) == 768\n",
    "    except TypeError:\n",
    "        print(type(row.embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embedding per emoji in tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(text):\n",
    "    return add_spaces_between_emojis(emoji.demojize(text))\n",
    "\n",
    "def add_spaces_between_emojis(demojified_text):\n",
    "    new_text = []\n",
    "    colons = []\n",
    "    for char in demojified_text:\n",
    "        if char == \":\":\n",
    "            if colons:\n",
    "                new_text.append(char + \" \")\n",
    "                colons.pop()\n",
    "            else:\n",
    "                colons.append(char)\n",
    "                new_text.append(\" \" + char)\n",
    "        else:\n",
    "            new_text.append(char)\n",
    "    return ''.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.tweet = tweets.tweet.parallel_apply(preprocess_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_emojis = pd.read_csv(AMBIGUITY_CLUSTER).emoji.unique()\n",
    "all_emojis = pd.read_csv(AMBIGUITY_PATH).emoji.unique()\n",
    "all_emojis = list(map(lambda x: emoji.demojize(x), all_emojis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 of our emojis are in this model\n",
      "Original number of tokens: 64001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens after extension: 64884\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "emojis_in_tokenizer = {}\n",
    "for em in all_emojis:\n",
    "    emoji_tensor = tokenizer(em, return_tensors='pt')['input_ids']\n",
    "    emoji_vocab_idx = emoji_tensor[0][1].item()\n",
    "    # if size is 3 it means emoji token was not splitted so it is a known token,\n",
    "    # [start] [emoji] [stop]\n",
    "    # index 3 stands for an unknown token\n",
    "    if emoji_tensor.size(1) == 3 and emoji_vocab_idx != 3:\n",
    "        emojis_in_tokenizer[em] = emoji_vocab_idx\n",
    "print(f\"{len(emojis_in_tokenizer)} of our emojis are in this model\")\n",
    "\n",
    "emojis_not_in_tokenizer = set(all_emojis) - set(emojis_in_tokenizer.keys())\n",
    "original_tokenizer_size = len(tokenizer)\n",
    "print(f\"Original number of tokens: {original_tokenizer_size}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",\\\n",
    "                                         additional_special_tokens=list(emojis_not_in_tokenizer))\n",
    "print(f\"Number of tokens after extension: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_emojis = pd.read_csv(AMBIGUITY_CLUSTER).emoji.unique()\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "# emojis_in_tokenizer = {}\n",
    "# for em in all_emojis:\n",
    "#     emoji_tensor = tokenizer(emoji.demojize(em), return_tensors='pt')['input_ids']\n",
    "#     emoji_vocab_idx = emoji_tensor[0][1].item()\n",
    "#     # if size is 3 it means emoji token was not splitted so it is a known token,\n",
    "#     # [start] [emoji] [stop]\n",
    "#     # index 3 stands for an unknown token\n",
    "#     if emoji_tensor.size(1) == 3 and emoji_vocab_idx != 3:\n",
    "#         emojis_in_tokenizer[em] = emoji_vocab_idx\n",
    "\n",
    "# print(f\"{len(emojis_in_tokenizer)} of our emojis are in this model\")\n",
    "# del all_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis_in_tokenizer_indices = set(emojis_in_tokenizer.values())\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def get_emoji_embedding(text):\n",
    "    tokenized = np.array(tokenizer.tokenize(text))\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    tokens_ids = encoded_input['input_ids']\n",
    "    mask = [id_.item() in emojis_in_tokenizer_indices \\\n",
    "            or id_.item() >= original_tokenizer_size for id_ in tokens_ids[0]]\n",
    "    if any(mask):\n",
    "        try:\n",
    "            features = model(**encoded_input)[0]\n",
    "        except IndexError:\n",
    "            return np.nan, np.nan\n",
    "        return features[0][mask][:].detach().numpy(), tokenized[mask[1:-1]]\n",
    "    else:\n",
    "        return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tweets.progress_apply(get_emoji_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[[\"embedding\", \"emoji\"]] = pd.DataFrame(out.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"embedding\"] = tweets[\"embedding\"].parallel_apply(lambda x: x.tolist())\n",
    "tweets[\"emoji\"] = tweets[\"emoji\"].parallel_apply(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.set_index(['tweet']).apply(pd.Series.explode).reset_index()\n",
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4fc18b230a4940ac20b527cbb70152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (188 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "save_to_csv(tweets, \"/scratch/czestoch/bert_emojis_with_unknown_emojis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emoji\n",
       ":1st_place_medal:            467\n",
       ":2nd_place_medal:            154\n",
       ":3rd_place_medal:            130\n",
       ":AB_button_(blood_type):     116\n",
       ":ATM_sign:                    63\n",
       "                            ... \n",
       ":zany_face:                 1135\n",
       ":zebra:                      225\n",
       ":zipper-mouth_face:           98\n",
       ":zombie:                      47\n",
       ":zzz:                        272\n",
       "Name: embedding, Length: 1193, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupby(\"emoji\").embedding.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if unknown tokens are rubbish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    features = model(**encoded_input)\n",
    "    return features[1].detach().cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(64003, 768)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",\\\n",
    "                                         additional_special_tokens=[\":paintbrush_selector:\",\\\n",
    "                                                                    emoji.demojize('‚ù§Ô∏è')])\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors are the same: False\n",
      "Difference between vectors: -0.24916860461235046\n"
     ]
    }
   ],
   "source": [
    "# print(f\"This is an initially known token: {emoji.demojize('üñåÔ∏è') in emojis_in_tokenizer}\")\n",
    "# print(f\"This is an initially known token: {emoji.demojize('‚ù§Ô∏è') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üñåÔ∏è\"\n",
    "text2 = \"This is amazing, trust me! ‚ù§Ô∏è\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "print(f\"Vectors are the same: {(one_pass == second_pass).all()}\")\n",
    "print(f\"Difference between vectors: {(one_pass - second_pass).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: False\n",
      "This is an initially known token: False\n",
      "Vectors are the same: False\n",
      "Difference between vectors: 1.4294824600219727\n"
     ]
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('üñåÔ∏è') in emojis_in_tokenizer}\")\n",
    "print(f\"This is an initially known token: {emoji.demojize('‚ù§Ô∏è') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üñåÔ∏è\"\n",
    "text2 = \"This is amazing, trust me! ‚ù§Ô∏è\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "print(f\"Vectors are the same: {(one_pass == second_pass).all()}\")\n",
    "print(f\"Difference between vectors: {(one_pass - second_pass).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: True\n",
      "This is an initially known token: True\n",
      "Vectors are the same: False\n",
      "Difference between vectors: -0.09044761955738068\n"
     ]
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('üòÇ') in emojis_in_tokenizer}\")\n",
    "print(f\"This is an initially known token: {emoji.demojize('üíì') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üòÇ\"\n",
    "text2 = \"This is amazing, trust me! üíì\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "print(f\"Vectors are the same: {(one_pass == second_pass).all()}\")\n",
    "print(f\"Difference between vectors: {(one_pass - second_pass).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(f\"This is an initially known token: {emoji.demojize('üñåÔ∏è') in emojis_in_tokenizer}\")\n",
    "# print(f\"This is an initially known token: {emoji.demojize('‚ù§Ô∏è') in emojis_in_tokenizer}\")\n",
    "text2 = \"This is amazing, trust me! üñåÔ∏è\"\n",
    "text2 = emoji.demojize(text2)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass == second_pass).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.47473258"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('üñåÔ∏è') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üñåÔ∏è\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it üñåÔ∏è\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Heart is initially not in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.48091918"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('‚ù§Ô∏è') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! ‚ù§Ô∏è\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it ‚ù§Ô∏è\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Emojis below are in the original vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.2963187"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('üòÇ') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üòÇ\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it üòÇ\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.20267165"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('üíì') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üíì\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it üíì\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an initially known token: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.60374236"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"This is an initially known token: {emoji.demojize('üß°') in emojis_in_tokenizer}\")\n",
    "text1 = \"This is amazing, trust me! üß°\"\n",
    "text2 = \"This is absolutely horrible, never ever try doing it üß°\"\n",
    "text1 = emoji.demojize(text1)\n",
    "text2 = emoji.demojize(text2)\n",
    "one_pass = get_embedding(text1)\n",
    "second_pass = get_embedding(text2)\n",
    "(one_pass - second_pass).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check if embeddinga on unknown tokens that were added to vocabulary are rubish\n",
    "### check which emojis are in the tokenizer and how to extract their embeddings later\n",
    "### check parallelization\n",
    "### save embeddings, yupi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
