{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bright-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import texthero as hero\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from p_tqdm import p_map\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain\n",
    "import pickle\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "plt.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chronic-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# os.environ[\"PYTHONPATH\"] = \"/home/czestoch/workspace/emoji-ambiguity/src\"\n",
    "from src.analysis.embedded import find_embedding\n",
    "from settings import AMBIGUITY_CLUSTER, AMBIGUITY_PATH, EMOJI_CATEGORIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "soviet-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api.info()['models'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "earned-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api.info()['models']['glove-twitter-200']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "agreed-theory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "      <th>space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>[0.0011291504, -0.00089645386, 0.00031852722, ...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>[0.0703125, 0.08691406, 0.087890625, 0.0625, 0...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for</td>\n",
       "      <td>[-0.011779785, -0.04736328, 0.044677734, 0.063...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that</td>\n",
       "      <td>[-0.01574707, -0.028320312, 0.083496094, 0.050...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>[0.0070495605, -0.07324219, 0.171875, 0.022583...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word                                          embedding space\n",
       "0  </s>  [0.0011291504, -0.00089645386, 0.00031852722, ...  news\n",
       "1    in  [0.0703125, 0.08691406, 0.087890625, 0.0625, 0...  news\n",
       "2   for  [-0.011779785, -0.04736328, 0.044677734, 0.063...  news\n",
       "3  that  [-0.01574707, -0.028320312, 0.083496094, 0.050...  news\n",
       "4    is  [0.0070495605, -0.07324219, 0.171875, 0.022583...  news"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating twitter space\n",
    "# word_embeddings = api.load('glove-twitter-200')\n",
    "### Creating google news space\n",
    "word_embeddings = api.load('word2vec-google-news-300')\n",
    "news_space = {\"word\": [], \"embedding\": [], \"space\": \"news\"}\n",
    "for word in word_embeddings.vocab.keys():\n",
    "    news_space[\"word\"].append(word)\n",
    "    news_space[\"embedding\"].append(word_embeddings.get_vector(word))\n",
    "news_space = pd.DataFrame(news_space)\n",
    "news_space.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "humanitarian-finance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing subset of words...\n",
      "Computing embeddings...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "      <th>space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>crazy</td>\n",
       "      <td>[0.030639648, -0.019165039, 0.03881836, 0.2226...</td>\n",
       "      <td>emoji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cornfield</td>\n",
       "      <td>[0.41992188, 0.047851562, 0.053955078, 0.02868...</td>\n",
       "      <td>emoji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>notebook</td>\n",
       "      <td>[-0.017089844, 0.15429688, -0.07910156, -0.012...</td>\n",
       "      <td>emoji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buds</td>\n",
       "      <td>[0.008972168, 0.36328125, -0.09716797, 0.05029...</td>\n",
       "      <td>emoji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>microscope</td>\n",
       "      <td>[-0.087890625, 0.24707031, 0.12011719, -0.1669...</td>\n",
       "      <td>emoji</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word                                          embedding  space\n",
       "0       crazy  [0.030639648, -0.019165039, 0.03881836, 0.2226...  emoji\n",
       "1   cornfield  [0.41992188, 0.047851562, 0.053955078, 0.02868...  emoji\n",
       "2    notebook  [-0.017089844, 0.15429688, -0.07910156, -0.012...  emoji\n",
       "3        buds  [0.008972168, 0.36328125, -0.09716797, 0.05029...  emoji\n",
       "4  microscope  [-0.087890625, 0.24707031, 0.12011719, -0.1669...  emoji"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating emoji space\n",
    "emojis = pd.read_csv(AMBIGUITY_PATH, encoding='utf-8')\n",
    "print(\"Choosing subset of words...\")\n",
    "vocabularies = emojis[[\"emoji\", \"word\"]] \\\n",
    "    .groupby(\"emoji\").word \\\n",
    "    .apply(list).apply(Counter) \\\n",
    "    .reset_index() \\\n",
    "    .rename({\"word\": \"vocabulary\"}, axis=1).set_index(\"emoji\")\n",
    "ambiguity_vocab = {st for row in vocabularies.vocabulary for st in row}\n",
    "print(\"Computing embeddings...\")\n",
    "tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "emoji_subspace = {\"word\": [], \"embedding\": [], \"space\": \"emoji\"}\n",
    "for emoji_description in ambiguity_vocab:\n",
    "    tokens = {token.text for token in tokenizer(emoji_description)}\n",
    "    vec = find_embedding(tokens, word_embeddings)\n",
    "    if vec is not None:\n",
    "        emoji_subspace[\"word\"].append(emoji_description)\n",
    "        emoji_subspace[\"embedding\"].append(vec)\n",
    "emoji_subspace = pd.DataFrame(emoji_subspace)\n",
    "emoji_subspace.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outer-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "del word_embeddings\n",
    "del emojis\n",
    "del ambiguity_vocab\n",
    "del vocabularies\n",
    "del tokens\n",
    "del vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-baseball",
   "metadata": {},
   "source": [
    "### Look at projection of emoji space only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_subspace['pca'] = hero.pca(emoji_subspace['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EMOJI_CATEGORIZED, \"rb\") as f:\n",
    "    emojis_categorized = pickle.load(f)\n",
    "\n",
    "categorized = {c: set(e) for c, e in emojis_categorized.items()}\n",
    "emojis = emojis[[\"emoji\", \"word\"]]\n",
    "emoji_subspace.word = emoji_subspace.word.astype(str)\n",
    "emojis.word = ambiguity.word.astype(str)\n",
    "emojis = emojis.drop_duplicates(\"word\")\n",
    "emoji_subspace1 = pd.merge(emoji_subspace, emojis, how='left', left_on='word', right_on='word')\n",
    "emoji_subspace1[\"category\"] = \"twitter\"\n",
    "\n",
    "\n",
    "def find_categories(df):\n",
    "    indices = []\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        for category in categorized:\n",
    "            if row.emoji in categorized[category]:\n",
    "                indices.append((idx, category))\n",
    "    return indices\n",
    "\n",
    "n_cores = 8\n",
    "df_split = np.array_split(emoji_subspace1, n_cores)\n",
    "out = list(chain.from_iterable(p_map(find_categories, df_split, num_cpus=n_cores))\n",
    "unzipped = list(zip(*out))\n",
    "indices = unzipped[0]\n",
    "categories = unzipped[1]\n",
    "\n",
    "emoji_subspace1.loc[indices, \"category\"] = categories\n",
    "emoji_subspace1[['pca_x','pca_y']] = pd.DataFrame(emoji_subspace1.pca.tolist(), index=emoji_subspace1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-gravity",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = emoji_subspace1[emoji_subspace1.category != 'flags']\n",
    "sns.jointplot(data=df, x='pca_x', y='pca_y', hue='category', kind='hist', height=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-combine",
   "metadata": {},
   "source": [
    "### Find projection of the whole space twitter + emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adjusted-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_space = pd.concat((news_space, emoji_subspace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "comparable-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "del news_space\n",
    "del emoji_subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole_space = whole_space[~whole_space.word.isin(['u', 'b', 'r', 'n', 'm'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_space['pca'] = hero.pca(whole_space['embedding'])\n",
    "whole_space[['pca_x','pca_y']] = pd.DataFrame(whole_space.pca.tolist(), index=whole_space.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=whole_space, x='pca_x', y='pca_y', hue='space', kind='hist', height=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "hero.scatterplot(\n",
    "    whole_space, \n",
    "    col='pca', \n",
    "    color='space',\n",
    "    hover_data=[\"word\"],\n",
    "    title=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-testing",
   "metadata": {},
   "source": [
    "### Look at the whole space with category division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import EMOJI_CATEGORIZED\n",
    "\n",
    "with open(EMOJI_CATEGORIZED, \"rb\") as f:\n",
    "    emojis_categorized = pickle.load(f)\n",
    "    \n",
    "ambiguity = pd.read_csv(\"/scratch/czestoch/ambiguity_dataset.csv.gz\")\n",
    "ambiguity = ambiguity[[\"emoji\", \"word\"]]\n",
    "\n",
    "categorized = {c:set(e) for c,e in emojis_categorized.items()}\n",
    "emojis = emojis[[\"emoji\", \"word\"]]\n",
    "\n",
    "whole_space.word = whole_space.word.astype(str)\n",
    "emojis.word = ambiguity.word.astype(str)\n",
    "emojis = emojis.drop_duplicates(\"word\")\n",
    "\n",
    "whole_space1 = pd.merge(whole_space, emojis, how='left', left_on='word', right_on='word')\n",
    "whole_space1[\"category\"] = \"twitter\"\n",
    "\n",
    "emoji_space = whole_space1[whole_space1.space == 'emoji']\n",
    "\n",
    "def find_categories(df):\n",
    "    indices = []\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        for category in categorized:\n",
    "            if row.emoji in categorized[category]:\n",
    "                indices.append((idx, category))\n",
    "    return indices\n",
    "\n",
    "n_cores = 8\n",
    "df_split = np.array_split(emoji_space, n_cores)\n",
    "out = p_map(find_categories, df_split, num_cpus=n_cores)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "to_set = list(chain.from_iterable(out))\n",
    "\n",
    "unzipped = list(zip(*to_set))\n",
    "indices = unzipped[0]\n",
    "categories = unzipped[1]\n",
    "\n",
    "whole_space1.loc[indices, \"category\"] = categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_space1[whole_space1.space == 'twitter'].category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(whole_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_space1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_space1.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['people', 'symbols', 'travel-places', 'objects',\n",
    "               'activity', 'nature', 'food-drink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hexbin(x, y, color, **kwargs):\n",
    "    cmap = sns.light_palette(color, as_cmap=True)\n",
    "    plt.hexbin(x, y, gridsize=15, cmap=cmap, **kwargs)\n",
    "\n",
    "# with sns.axes_style(\"dark\"):\n",
    "df = whole_space1[(whole_space1.space =='emoji') & (whole_space1.category != 'flags')]\n",
    "g = sns.FacetGrid(df, hue=\"category\", col=\"category\", col_wrap=3, height=4)\n",
    "g.map(hexbin, \"pca_x\", \"pca_y\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = whole_space1[(whole_space1.space =='emoji') & (whole_space1.category != 'flags')]\n",
    "sns.jointplot(data=df, x='pca_x', y='pca_y', hue='category', kind='hist', height=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=whole_space1, x='pca_x', y='pca_y', hue='category', kind='hist', height=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-running",
   "metadata": {},
   "source": [
    "### Analyze the pca output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-temperature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-roots",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/czestoch/tsne.pkl\", \"rb\") as f:\n",
    "    whole_space = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_space.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(whole_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=whole_space, x='tsne_x', y='tsne_y', hue='space', kind='hist', height=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_space = whole_space[['word', 'space', 'tsne_x', 'tsne_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import EMOJI_CATEGORIZED\n",
    "\n",
    "with open(EMOJI_CATEGORIZED, \"rb\") as f:\n",
    "    emojis_categorized = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized = {c:set(e) for c,e in emojis_categorized.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguity = pd.read_csv(\"/scratch/czestoch/ambiguity_dataset.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguity = ambiguity[[\"emoji\", \"word\"]]\n",
    "ambiguity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import EMOJI_CATEGORIZED\n",
    "\n",
    "with open(EMOJI_CATEGORIZED, \"rb\") as f:\n",
    "    emojis_categorized = pickle.load(f)\n",
    "    \n",
    "ambiguity = pd.read_csv(\"/scratch/czestoch/ambiguity_dataset.csv.gz\")\n",
    "ambiguity = ambiguity[[\"emoji\", \"word\"]]\n",
    "\n",
    "categorized = {c:set(e) for c,e in emojis_categorized.items()}\n",
    "emojis = emojis[[\"emoji\", \"word\"]]\n",
    "\n",
    "whole_space.word = whole_space.word.astype(str)\n",
    "emojis.word = ambiguity.word.astype(str)\n",
    "emojis = emojis.drop_duplicates(\"word\")\n",
    "\n",
    "whole_space1 = pd.merge(whole_space, emojis, how='left', left_on='word', right_on='word')\n",
    "whole_space1[\"category\"] = \"twitter\"\n",
    "\n",
    "emoji_space = whole_space1[whole_space1.space == 'emoji']\n",
    "\n",
    "def find_categories(df):\n",
    "    indices = []\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        for category in categorized:\n",
    "            if row.emoji in categorized[category]:\n",
    "                indices.append((idx, category))\n",
    "    return indices\n",
    "\n",
    "n_cores = 8\n",
    "df_split = np.array_split(emoji_space, n_cores)\n",
    "out = p_map(find_categories, df_split, num_cpus=n_cores)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "to_set = list(chain.from_iterable(out))\n",
    "\n",
    "unzipped = list(zip(*to_set))\n",
    "indices = unzipped[0]\n",
    "categories = unzipped[1]\n",
    "\n",
    "whole_space1.loc[indices, \"category\"] = categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=whole_space, x='tsne_x', y='tsne_y', hue='category', kind='hist', height=10);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
